{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import math\n",
    "import os\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding:utf-8 -*-\n",
    "import requests\n",
    "import json\n",
    "import re\n",
    "import os\n",
    "import urllib\n",
    "import lxml\n",
    "import time\n",
    "import datetime\n",
    "from urllib import request,parse\n",
    "from bs4 import BeautifulSoup\n",
    " \n",
    " \n",
    "def Baidu_spider(search_word,i):\n",
    "    # 创建文件夹\n",
    "    # folders = os.path.exists('F:\\\\SPIDER_DATA\\\\img_real\\\\%s' %img_group)\n",
    "    # if not folders:\n",
    "    #     os.makedirs(\"image\")\n",
    "\n",
    "    keyword = search_word   # 爬取关键字\n",
    "    INIT =1  ##从第几页开始爬取\n",
    "    N = 1 ##爬取的页数(每页20张图\n",
    "    tic = time.time()\n",
    "    url = crawler_img(keyword,INIT,N,i)\n",
    "    toc = time.time()\n",
    "    print(\"总计耗时：%s秒\" % (toc - tic))\n",
    "    return url\n",
    "    # print(\"爬取范围：第%s页 到 第%s页\" %(INIT,N-1+INIT))\n",
    "\n",
    "def crawler_img(keyword,INIT,N,i):\n",
    "    keyword_encode = parse.quote(keyword)  # 可以把字符串编码为url的格式\n",
    "    each_page_num = 5  ##总共爬多少个\n",
    "   \n",
    "    for page in range(INIT-1,INIT-1+N):\n",
    "        pn = page*20\n",
    "        url = 'http://image.baidu.com/search/flip?tn=baiduimage&ie=utf-8&word=' + keyword_encode + '&pn=%s' %pn\n",
    "        url_request = request.Request(url)\n",
    "        url_response = request.urlopen(url_request)  # 请求数据，可以和上一句合并\n",
    "        html = url_response.read().decode('utf-8')  # 加编码，重要！转换为字符串编码，read()得到的是byte格式的。\n",
    "        jpgList = re.findall('\"objURL\":\"(.*?)\",',html,re.S)  #re.S将字符串作为整体，在整体中进行正则匹配\n",
    "        n = 1\n",
    "        img_url = []\n",
    "        for each in jpgList:\n",
    "            # print(each)  # 每个图片的下载地址\n",
    "            try:\n",
    "                # print(n+pn)  # 图片编号\n",
    "                print(\"正在下载 %s - %s / %s ...\" %(keyword, n+pn, each_page_num*N))\n",
    "                resp = requests.get(each, timeout=10)\n",
    "                if resp.status_code != 200:\n",
    "                    raise RuntimeError(f'Response status = {resp.status_code}')\n",
    "                with open('F:\\\\SPIDER_DATA\\\\img_fake\\\\document_fake\\\\%s.jpg' %(i), 'wb') as f:\n",
    "                    f.write(resp.content)\n",
    "                # request.urlretrieve(each,'C:\\\\Users\\\\WIN10\\\\Desktop\\\\url_dataset\\\\%s\\\\%s\\\\%s.jpg' %(img_group,keyword,n+pn))\n",
    "                img_url.append(each)\n",
    "                break\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                print(\"正在下载 %s / %s ...\" % (n + pn , each_page_num * N))\n",
    "                print(\"第%s张图片下载失败\" %(n+pn))\n",
    "            n = n+1\n",
    "            if n > each_page_num:####每页下载多少图片\n",
    "                break\n",
    "    return img_url\n",
    "    # print(\"下载完成！\")\n",
    " \n",
    " \n",
    "# if __name__ == \"__main__\":\n",
    " \n",
    "#     image_url = Baidu_spider('中国人将接受全球首例换头手术','shanghai_img')###输入要查询的keyword和其所在网站的名字，例'shanghai'\n",
    "#     # print(image_url)\n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "####down_fake_data\n",
    "fileName=r\"F:\\SPIDER_DATA\\Fengshenbang-LM-main\\720_www_fake_data.xlsx\"\n",
    "data=pd.read_excel(fileName)\n",
    "# folder_path = r'F:\\SPIDER_DATA\\img_real\\claim_real'\n",
    "# data['num']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 这块是筛选img序号的图片document是否能打开\n",
    "clean_title = []\n",
    "clean_text = []\n",
    "clean_image = []\n",
    "clean_num = []\n",
    "\n",
    "x = 0\n",
    "\n",
    "folder_path = r'F:\\SPIDER_DATA\\img_fake\\claim_fake'\n",
    "\n",
    "for i,ti in enumerate(data['title']):\n",
    "    num = data['num'][i]\n",
    "    te = data['text'][i]\n",
    "    img_index = data['image'][i]\n",
    "    summary = data['summary'][i]\n",
    "    # if num == 143:\n",
    "    #     print(img)\n",
    "    #     print(type(img))\n",
    "    try:\n",
    "        img = Image.open(os.path.join(folder_path, str(img_index)))  # 尝试打开图片\n",
    "        img.verify()  # 验证图片\n",
    "        clean_title.append(ti)\n",
    "        clean_text.append(te)\n",
    "        clean_image.append(img_index)\n",
    "        clean_num.append(num)\n",
    "\n",
    "    except (IOError, SyntaxError) as e:  # 捕获异常\n",
    "        print(f\"不能打开的文件：{img_index}\")\n",
    "        # 爬取图片\n",
    "        # flag = Baidu_spider(str('fake news'),num)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_title = []\n",
    "clean_text = []\n",
    "clean_image = []\n",
    "clean_num = []\n",
    "\n",
    "x = 0\n",
    "\n",
    "folder_path = r'F:\\SPIDER_DATA\\img_real\\claim_real'\n",
    "\n",
    "for i,ti in enumerate(data['title']):\n",
    "    num = data['num'][i]\n",
    "    te = data['text'][i]\n",
    "    img_index = data['image'][i]\n",
    "    \n",
    "    # if num == 143:\n",
    "    #     print(img)\n",
    "    #     print(type(img))\n",
    "    try:\n",
    "        img = Image.open(os.path.join(folder_path, str(img_index)))  # 尝试打开图片\n",
    "        img.verify()  # 验证图片\n",
    "        clean_title.append(ti)\n",
    "        clean_text.append(te)\n",
    "        clean_image.append(img_index)\n",
    "        clean_num.append(num)\n",
    "\n",
    "    except (IOError, SyntaxError) as e:  # 捕获异常\n",
    "        print(f\"不能打开的文件：{img_index}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.DataFrame({'num':clean_num,'title':clean_title,'text':clean_text,'image':clean_image}) #,'cate':clean_cate\n",
    "data.to_csv('clean_www_real_data.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('PyTorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "330807cf3b03783e20e51dbe2885055e79447f405d415e2682cdd78f6af61837"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
